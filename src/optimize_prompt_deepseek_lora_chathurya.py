# -*- coding: utf-8 -*-
"""optimize_prompt_deepseek_lora_chathurya.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-tI-FCpY8EULOv5ZcvCDV0qQNyS-baVH
"""

!pip install -q transformers datasets peft accelerate

import torch, os, json, shutil
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from datasets import Dataset as HFDataset
from google.colab import files

print("Starting DeepSeek LoRA Fine-tuning …")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

print("\n" + "="*50)
print("Please upload your dataset.jsonl file:")
print("="*50)

uploaded = files.upload()
dataset_file = None
for fname in uploaded:
    if fname.endswith(".jsonl"):
        if fname != "dataset.jsonl":
            os.rename(fname, "dataset.jsonl")
        dataset_file = "dataset.jsonl"
        break
if dataset_file is None:
    raise ValueError("No .jsonl file uploaded!")

with open(dataset_file) as f:
    rows = [ln for ln in f if ln.strip()]
print(f"\n✓ Uploaded {dataset_file}  — {len(rows)} examples")

data = []
with open(dataset_file) as f:
    for idx, ln in enumerate(f, 1):
        if ln.strip():
            try:
                data.append(json.loads(ln))
            except json.JSONDecodeError as e:
                print("Skip line", idx, e)
dataset = HFDataset.from_list(data)
print("Columns:", dataset.column_names)

model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

special_tokens = {
    "additional_special_tokens": [
        "<disaster_analysis>", "</disaster_analysis>",
        "<event_summary>", "</event_summary>",
        "<detailed_analysis>", "</detailed_analysis>",
        "<predictions>", "</predictions>",
        "<impacts>", "</impacts>",
        "<mitigation_strategies>", "</mitigation_strategies>",
        "<recommendations>", "</recommendations>",
    ]
}
tokenizer.add_special_tokens(special_tokens)

model = AutoModelForCausalLM.from_pretrained(
    model_name, torch_dtype=torch.float16, device_map="auto"
)
model.resize_token_embeddings(len(tokenizer))
print("Model & tokenizer loaded")

lora_cfg = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

system_prompt = (
    "You are an advanced disaster response language model trained to generate complete, structured reports from brief instructions. "
    "Every response must strictly follow this output format, with exactly seven sections using the specified tags: "
    "<disaster_analysis>, <event_summary>, <detailed_analysis>, <predictions>, <impacts>, <mitigation_strategies>, and <recommendations>. "
    "Begin your response with <disaster_analysis> and continue in this order. "
    "\n\n"
    "<disaster_analysis>: Provide a thorough analysis covering 13 specific dimensions: (1) geographic location, (2) disaster type, (3) magnitude/severity, "
    "(4) time of occurrence, (5) weather conditions, (6) human exposure and vulnerability, (7) infrastructure risks, (8) historical frequency, "
    "(9) economic sensitivity, (10) critical resources affected, (11) communication systems, (12) emergency preparedness, (13) geopolitical impact. "
    "\n<event_summary>: One-paragraph summary explaining what the disaster is, when/where it happened, and its immediate consequence. "
    "\n<detailed_analysis>: Explain the cause, spread, affected populations/areas, and relevant statistics (if applicable). "
    "\n<predictions>: Predict how the disaster may evolve, impact escalation, and secondary threats (e.g., power outages, disease, aftershocks). "
    "\n<impacts>: List specific and measurable impacts on people, infrastructure, environment, economy, and transportation. "
    "\n<mitigation_strategies>: Suggest short-term and long-term steps to reduce risk, improve readiness, and recover. Include responsible parties. "
    "\n<recommendations>: Give actionable advice to government, responders, NGOs, and civilians. Do not repeat prior sections. "
    "\n\nDo not include introductions, explanations, or JSON. Do not mention you're an AI. Use only the 7 tags in correct order. "
    "Always start with <disaster_analysis> and ensure every section is present in every response."
)

def format_batch(ex):
    text = [
        f"{system_prompt}\n\nInstruction: {i}\n\nResponse:\n{o}"
        for i, o in zip(ex["instruction"], ex["output"])
    ]
    tok = tokenizer(
        text, max_length=1024, padding="max_length", truncation=True, return_tensors="pt"
    )
    tok["labels"] = tok["input_ids"].clone()
    return tok

tok_ds = dataset.map(
    format_batch,
    batched=True,
    remove_columns=dataset.column_names,
    desc="Tokenising dataset",
)

training_args = TrainingArguments(
    output_dir="./deepseek-lora-output",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    warmup_steps=10,
    learning_rate=2e-5,
    fp16=torch.cuda.is_available(),
    logging_steps=5,
    save_steps=20,
    save_total_limit=2,
    remove_unused_columns=False,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tok_ds,
    tokenizer=tokenizer,
)

steps = (len(tok_ds) //
         (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)
         * training_args.num_train_epochs)
print("Total training steps:", steps)

torch.cuda.empty_cache()
trainer.train()
print("✓ Training complete")

out_dir = "./deepseek-lora-final"
model.save_pretrained(out_dir)
tokenizer.save_pretrained(out_dir)
print("Saved to", out_dir)

print("\n" + "="*50)
print("Testing model (forced tag prefix)…")
print("="*50)

required_tags = [
    "<disaster_analysis>", "<event_summary>", "<detailed_analysis>",
    "<predictions>", "<impacts>", "<mitigation_strategies>", "<recommendations>",
]

def gen(instr, max_new=1200):
    prompt = f"{system_prompt}\n\nInstruction: {instr}\n\nResponse:\n<disaster_analysis>"
    inp = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        out = model.generate(
            **inp,
            max_new_tokens=max_new,
            temperature=0.2,
            top_p=0.8,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    return tokenizer.decode(out[0], skip_special_tokens=False)

tests = [
    "Analyze the potential impact of a Category 5 hurricane hitting Miami, Florida.",
    "Evaluate the risks and provide analysis for a major earthquake in Los Angeles.",
    "Assess the disaster preparedness for flooding in a coastal city.",
]

for n, t in enumerate(tests, 1):
    print(f"\nTest {n}: {t}")
    print("-"*110)
    reply = gen(t)
    print(reply[:2000] + (" …[truncated]" if len(reply) > 2000 else ""))

    found = [tag for tag in required_tags if tag in reply]
    print(f"\n✓ Found {len(found)}/{len(required_tags)} tags → {found}")

# zip_name = "deepseek-lora-final"
# shutil.make_archive(zip_name, "zip", out_dir)
# files.download(zip_name + ".zip")
# print("Model zipped & download triggered")
